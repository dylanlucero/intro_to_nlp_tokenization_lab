{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5dd4e5d",
   "metadata": {},
   "source": [
    "# Intro to NLP Lab: Tokenization and Vocabulary Size vs Sequence Length\n",
    "\n",
    "Welcome to this lab assignment on **tokenization**! Tokenization is the process of breaking text up into smaller units (tokens) that can be processed by a computer. In this lab, you'll implement and compare four tokenization strategies — **character-level**, **word-level**, **phrase-level (using bigrams)**, and **subword tokenization using Byte-Pair Encoding (BPE)** — and explore how each approach affects the size of the vocabulary and the length of tokenized sequences.\n",
    "\n",
    "By the end of this lab you should be able to:\n",
    "\n",
    "- Implement simple character- and word-level tokenizers.\n",
    "- Compute bigram frequencies and implement a phrase-level tokenizer that merges common bigrams.\n",
    "- Implement a basic Byte-Pair Encoding (BPE) algorithm for subword tokenization.\n",
    "- Measure and visualize how different tokenization strategies trade off vocabulary size and sequence length.\n",
    "- Reflect on why naive whitespace tokenization is often insufficient.\n",
    "\n",
    "The entire lab should take about **1–2 hours** to complete. Have fun!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532fcd24",
   "metadata": {},
   "source": [
    "## Sample Text\n",
    "\n",
    "For this lab we'll use a longer excerpt from *Alice's Adventures in Wonderland* (Chapter 1) as our corpus. The excerpt is provided in the file `sample_text.txt` in the lab directory. The text is adapted from the public domain version of the story【949699819888551†L4-L84】.\n",
    "\n",
    "**Please do not modify the contents of this file.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce650be",
   "metadata": {},
   "source": [
    "## 1. Preprocessing\n",
    "\n",
    "Before tokenizing, we'll do a minimal amount of preprocessing. We will:\n",
    "\n",
    "- Convert the text to lowercase so that `Alice` and `alice` become the same token.\n",
    "- Replace newline characters (`\n",
    "`) with spaces so that the corpus forms one continuous sequence of tokens.\n",
    "- Leave punctuation intact (punctuation will remain attached to the nearest word) for simplicity.\n",
    "\n",
    "### Task 1\n",
    "\n",
    "Implement a function `preprocess_text(text: str) -> str` that performs these steps and returns the cleaned text. A few examples are given in the docstring. Do **not** perform any other preprocessing (e.g. do **not** strip punctuation or collapse multiple spaces).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981fbe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"Preprocess the input text by lowercasing and replacing newlines with spaces.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        Raw text from the corpus, potentially containing newlines.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A cleaned, lowercase string with newlines replaced by single spaces.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> preprocess_text('Hello\\nWorld!')\n",
    "    'hello world!'\n",
    "    >>> preprocess_text('A\\nB\\nC')\n",
    "    'a b c'\n",
    "    \"\"\"\n",
    "    # TODO: implement preprocessing (lowercase and replace newlines with spaces)\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8946e9ec",
   "metadata": {},
   "source": [
    "## 2. Character-Level Tokenization\n",
    "\n",
    "In a character-level tokenizer, each character becomes its own token. Spaces and punctuation are treated as individual tokens as well. This strategy yields the **smallest vocabulary** (all possible characters that appear in the corpus) but produces **very long sequences** because every character is a separate token.\n",
    "\n",
    "### Task 2\n",
    "\n",
    "Implement a function `char_tokenize(text: str) -> list[str]` that converts the input string into a list of characters. Your function should **not** strip spaces or punctuation; simply return a list containing each character in order.\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    ">>> char_tokenize('abc')\n",
    "['a', 'b', 'c']\n",
    ">>> char_tokenize('a b')\n",
    "['a', ' ', 'b']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ecf343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_tokenize(text: str) -> list[str]:\n",
    "    \"\"\"Tokenize the input string into individual characters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        A preprocessed string (lowercase with spaces instead of newlines).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[str]\n",
    "        A list of characters, including spaces and punctuation.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> char_tokenize('abc')\n",
    "    ['a', 'b', 'c']\n",
    "    >>> char_tokenize('a b')\n",
    "    ['a', ' ', 'b']\n",
    "    \"\"\"\n",
    "    # TODO: implement character-level tokenization\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1208c6",
   "metadata": {},
   "source": [
    "## 3. Word-Level Tokenization\n",
    "\n",
    "The simplest word tokenizer splits a string on whitespace characters. This strategy treats each word (separated by spaces) as a token and keeps punctuation attached to the nearest word. While easy to implement, it often results in a **very large vocabulary** because every inflected form (e.g. `rabbits` vs. `rabbit`) is considered a separate token.\n",
    "\n",
    "### Task 3\n",
    "\n",
    "Implement a function `word_tokenize(text: str) -> list[str]` that splits the input string on whitespace. Your function should preserve punctuation attached to words (e.g. `Rabbit!` should remain `Rabbit!`). Do **not** perform any additional preprocessing here; the input will already be cleaned by `preprocess_text`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b5b73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(text: str) -> list[str]:\n",
    "    \"\"\"Tokenize the input string on whitespace.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        A preprocessed string with lowercase letters and spaces.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[str]\n",
    "        A list of word tokens.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> word_tokenize('hello world!')\n",
    "    ['hello', 'world!']\n",
    "    >>> word_tokenize('multiple   spaces here')\n",
    "    ['multiple', 'spaces', 'here']\n",
    "    \"\"\"\n",
    "    # TODO: implement word-level tokenization\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fca457",
   "metadata": {},
   "source": [
    "## 4. Phrase-Level Tokenization via Bigrams\n",
    "\n",
    "A phrase-level tokenizer can merge frequently occurring bigrams (pairs of adjacent words) into single tokens. For example, the phrase `white rabbit` appears multiple times in our sample text; treating it as a single token `white_rabbit` can reduce sequence length but increases the vocabulary size.\n",
    "\n",
    "To implement this, we'll first compute a frequency dictionary of all bigrams in the corpus. Then we'll select the top *N* bigrams (we'll use **5** in this lab) and merge those pairs everywhere they occur.\n",
    "\n",
    "### Task 4\n",
    "\n",
    "1. Implement a function `compute_bigram_frequencies(tokens: list[str]) -> dict[tuple[str,str], int]` that returns a dictionary mapping each consecutive token pair to its frequency in the list.\n",
    "2. Implement a function `phrase_tokenize(tokens: list[str], bigrams: list[tuple[str,str]]) -> list[str]` that merges any bigram in `bigrams` into a single token joined by an underscore (e.g. `('white','rabbit')` becomes `'white_rabbit'`). Your function should scan through the token list from left to right, merging greedily when a listed bigram is found.\n",
    "\n",
    "Hints:\n",
    "- Use a loop to walk through the tokens. When you see a bigram to merge, append the merged token and skip the next word.\n",
    "- Otherwise, append the current token and move on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e365d152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bigram_frequencies(tokens: list[str]) -> dict[tuple[str, str], int]:\n",
    "    \"\"\"Compute frequencies of adjacent token pairs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens : list[str]\n",
    "        A list of tokens (words) from the corpus.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[tuple[str,str], int]\n",
    "        A dictionary mapping each bigram (pair of tokens) to its count.\n",
    "    \"\"\"\n",
    "    # TODO: implement bigram frequency counting\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "def phrase_tokenize(tokens: list[str], bigrams: list[tuple[str, str]]) -> list[str]:\n",
    "    \"\"\"Merge selected bigrams into single phrase tokens.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens : list[str]\n",
    "        A list of word tokens.\n",
    "    bigrams : list[tuple[str,str]]\n",
    "        A list of bigrams (tuples of two tokens) to merge.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[str]\n",
    "        The token list with specified bigrams merged into single tokens joined by an underscore.\n",
    "    \"\"\"\n",
    "    # TODO: implement phrase-level tokenization using the provided bigrams\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ff8987",
   "metadata": {},
   "source": [
    "## 5. Subword Tokenization via Byte-Pair Encoding (BPE)\n",
    "\n",
    "BPE is an unsupervised algorithm that gradually merges the most frequent pairs of symbols in a corpus until a desired vocabulary size is reached. In the context of tokenization, we treat each **word** as a sequence of characters plus a special end-of-word symbol `</w>`. At each iteration, we:\n",
    "\n",
    "1. Count the frequency of all adjacent symbol pairs (e.g. `('a', 'l')` in `alice</w>`).\n",
    "2. Find the most frequent pair and merge it everywhere in the corpus.\n",
    "3. Add this merged pair to our list of merges.\n",
    "\n",
    "After `num_merges` iterations, the learned merges define a subword vocabulary. To encode a new word, we repeatedly apply the merges from longest to shortest: the longest possible merged pairs are replaced first.\n",
    "\n",
    "This algorithm allows rare words to be represented as sequences of subword units, enabling the model to handle out-of-vocabulary words gracefully.\n",
    "\n",
    "### Task 5\n",
    "\n",
    "1. Implement a function `bpe_train(corpus: list[str], num_merges: int) -> list[tuple[str, str]]` that trains a BPE tokenizer on a list of word tokens (the **full corpus**, not just unique words). It should return the list of merges in the order they were applied.\n",
    "2. Implement a function `bpe_encode(word: str, merges: list[tuple[str, str]]) -> list[str]` that encodes a single word into a list of subword tokens by repeatedly applying the learned merges. Remember to remove the end-of-word symbol in the final output.\n",
    "\n",
    "Hints:\n",
    "- Represent each word as a list of characters followed by `</w>` during training and encoding.\n",
    "- At each merge step, replace occurrences of the target pair with their concatenation.\n",
    "- During encoding, apply merges sequentially in the order learned during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81277a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpe_train(corpus: list[str], num_merges: int) -> list[tuple[str, str]]:\n",
    "    \"\"\"Train a simple BPE tokenizer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus : list[str]\n",
    "        A list of word tokens in the corpus. Each word will be represented as a list of characters plus an end-of-word symbol during training.\n",
    "    num_merges : int\n",
    "        The number of pair merges to perform.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[tuple[str, str]]\n",
    "        The list of merged symbol pairs in the order they were applied.\n",
    "    \"\"\"\n",
    "    # TODO: implement BPE training on the full corpus\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "def bpe_encode(word: str, merges: list[tuple[str, str]]) -> list[str]:\n",
    "    \"\"\"Encode a single word using a trained BPE tokenizer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    word : str\n",
    "        A word to encode.\n",
    "    merges : list[tuple[str, str]]\n",
    "        The list of merges learned during BPE training.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[str]\n",
    "        A list of subword tokens representing the input word. The end-of-word symbol should **not** appear in the returned list.\n",
    "    \"\"\"\n",
    "    # TODO: implement BPE encoding using the provided merges\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b0ef7e",
   "metadata": {},
   "source": [
    "## 6. Evaluation\n",
    "\n",
    "With your four tokenizers defined, it's time to evaluate them. We'll measure two simple statistics:\n",
    "\n",
    "- **Vocabulary size:** the number of unique tokens produced by each tokenizer.\n",
    "- **Average sequence length:** the average number of tokens per sentence in the corpus.\n",
    "\n",
    "### Task 6\n",
    "\n",
    "1. Read in `sample_text.txt` and run `preprocess_text` on it.\n",
    "2. Tokenize the cleaned text using `char_tokenize`, `word_tokenize`, `phrase_tokenize` and your BPE tokenizer. For phrase-level tokenization, compute bigram frequencies, select the top **5** most frequent bigrams, and merge them. For BPE, train on the **full list of word tokens** with `num_merges=50` and then encode every word in the corpus.\n",
    "3. Compute the vocabulary size and average sequence length (number of tokens divided by number of sentences) for each tokenizer. Use the period `'.'` as a simple sentence delimiter. If the corpus contains no periods, treat it as one sentence.\n",
    "4. Print out the results and create a scatter plot of vocabulary size vs. average sequence length. Label each point with the tokenizer name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcce8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Load and preprocess the corpus\n",
    "with open('sample_text.txt', 'r', encoding='utf-8') as f:\n",
    "    sample_text = f.read()\n",
    "clean_text = preprocess_text(sample_text)\n",
    "\n",
    "# Compute the number of sentences (use period as delimiter)\n",
    "num_sentences = clean_text.count('.') if clean_text.count('.') > 0 else 1\n",
    "\n",
    "# 2. Tokenize using the different strategies\n",
    "char_tokens = char_tokenize(clean_text) \n",
    "word_tokens = word_tokenize(clean_text)\n",
    "\n",
    "# Phrase-level tokenization\n",
    "bigram_freqs = compute_bigram_frequencies(word_tokens)\n",
    "# Select the top 5 bigrams by frequency\n",
    "bigram_counter = Counter(bigram_freqs)\n",
    "top_bigrams = [bigram for bigram, _ in bigram_counter.most_common(5)]\n",
    "phrase_tokens = phrase_tokenize(word_tokens, top_bigrams)\n",
    "\n",
    "# BPE-level tokenization\n",
    "# Train BPE on the full list of word tokens\n",
    "merges = bpe_train(word_tokens, 50)\n",
    "bpe_tokens_corpus = []\n",
    "for w in word_tokens:\n",
    "    bpe_tokens_corpus.extend(bpe_encode(w, merges))\n",
    "\n",
    "# 3. Compute metrics\n",
    "# Vocabulary sizes\n",
    "char_vocab_size = len(set(char_tokens))\n",
    "word_vocab_size = len(set(word_tokens))\n",
    "phrase_vocab_size = len(set(phrase_tokens))\n",
    "bpe_vocab_size = len(set(bpe_tokens_corpus))\n",
    "\n",
    "# Average sequence lengths\n",
    "avg_char_length = len(char_tokens) / num_sentences\n",
    "avg_word_length = len(word_tokens) / num_sentences\n",
    "avg_phrase_length = len(phrase_tokens) / num_sentences\n",
    "avg_bpe_length = len(bpe_tokens_corpus) / num_sentences\n",
    "\n",
    "# Print results\n",
    "print('Character tokenizer: vocab size =', char_vocab_size, ', avg sequence length =', avg_char_length)\n",
    "print('Word tokenizer: vocab size =', word_vocab_size, ', avg sequence length =', avg_word_length)\n",
    "print('Phrase tokenizer: vocab size =', phrase_vocab_size, ', avg sequence length =', avg_phrase_length)\n",
    "print('BPE tokenizer: vocab size =', bpe_vocab_size, ', avg sequence length =', avg_bpe_length)\n",
    "\n",
    "# 4. Visualization\n",
    "labels = ['Character', 'Word', 'Phrase', 'BPE']\n",
    "vocab_sizes = [char_vocab_size, word_vocab_size, phrase_vocab_size, bpe_vocab_size]\n",
    "avg_lengths = [avg_char_length, avg_word_length, avg_phrase_length, avg_bpe_length]\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(vocab_sizes, avg_lengths)\n",
    "for i, label in enumerate(labels):\n",
    "    plt.annotate(label, (vocab_sizes[i], avg_lengths[i]))\n",
    "plt.title('Vocabulary Size vs. Average Sequence Length')\n",
    "plt.xlabel('Vocabulary size')\n",
    "plt.ylabel('Average sequence length')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245646f4",
   "metadata": {},
   "source": [
    "## 7. Reflection\n",
    "\n",
    "In a few sentences, discuss the trade‑offs you observed between vocabulary size and sequence length for the four tokenization strategies. Why might one prefer a subword tokenizer (like BPE) over purely character‑ or word‑level tokenization? How does phrase‑level tokenization compare? Your answer should be at least a few sentences long.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fa6876",
   "metadata": {},
   "source": [
    "*Your answer here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91126ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Hidden tests (autograder)\n",
    "# These tests will be used by the grading script to evaluate your solutions.\n",
    "# Do not modify this cell! If you do, you may receive zero credit.\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Test preprocess_text\n",
    "assert preprocess_text('Hello\\nWorld!') == 'hello world!'\n",
    "assert preprocess_text('A\\nB\\nC') == 'a b c'\n",
    "\n",
    "# Test char_tokenize\n",
    "assert char_tokenize('abc') == ['a','b','c']\n",
    "assert char_tokenize('a b') == ['a',' ','b']\n",
    "\n",
    "# Test word_tokenize\n",
    "assert word_tokenize('hello world!') == ['hello','world!']\n",
    "assert word_tokenize('multiple   spaces here') == ['multiple','spaces','here']\n",
    "\n",
    "# Test compute_bigram_frequencies\n",
    "bigrams_test = compute_bigram_frequencies(['the','cat','sat','on','the','mat'])\n",
    "assert bigrams_test.get(('the','cat'),0) == 1 and bigrams_test.get(('cat','sat'),0) == 1\n",
    "\n",
    "# Test phrase_tokenize\n",
    "phrase_test = phrase_tokenize(['the','white','rabbit','ran'], [('white','rabbit')])\n",
    "assert phrase_test == ['the','white_rabbit','ran']\n",
    "\n",
    "# Test BPE training on a tiny corpus\n",
    "simple_corpus = ['low','lowest','newest','wider']\n",
    "merges_test = bpe_train(simple_corpus, num_merges=10)\n",
    "# The first merge in standard BPE for this corpus should be ('e','s') or ('l','o'), depending on frequency ties.\n",
    "assert ('l','o') in merges_test or ('e','s') in merges_test\n",
    "\n",
    "# Test BPE encoding on a simple word\n",
    "encoded = bpe_encode('lowest', [('l','o'),('lo','w'),('e','s'),('es','t')])\n",
    "assert isinstance(encoded, list) and len(encoded) > 0\n",
    "\n",
    "print('All preliminary tests passed!')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
